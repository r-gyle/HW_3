{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(pd.read_csv('Data/data.csv', header=None))\n",
    "labels = np.ravel(pd.read_csv('Data/label.csv', header=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_type(X, similarity):\n",
    "    if similarity == 'cosine':\n",
    "        X = pairwise_distances(X, metric='cosine')\n",
    "    elif similarity == 'jaccard':\n",
    "        X = pairwise_distances(X, metric='hamming')\n",
    "    return X\n",
    "\n",
    "class KMeans:\n",
    "    def __init__(self, k, similarity='euclidean', stop='none') -> None:\n",
    "        self.k = k\n",
    "        self.similarity = similarity\n",
    "        self.stop = stop\n",
    "        self.centroids = None\n",
    "        self.c_labels = None\n",
    "        self.prev_score = 0.0\n",
    "        self.sse = 0.0\n",
    "        self.num_iter = 0\n",
    "\n",
    "    def calc_sse(self, X, y):\n",
    "        for i, centroid in enumerate(self.centroids):\n",
    "            cluster_points = X[y == i]\n",
    "            self.sse += np.sum((cluster_points - centroid) ** 2)\n",
    "        return self.sse\n",
    "    \n",
    "    def cluster_labels(self, temp, true):\n",
    "        c_labels = np.zeros(len(temp), dtype=int)\n",
    "        for cluster in range(100):\n",
    "            indices = np.where(temp == cluster)[0]\n",
    "            c_labels[indices] = np.argmax([np.sum(true[indices] == label) for label in np.unique(true)])\n",
    "        return c_labels\n",
    "\n",
    "    def fit(self, X, y, iterations=100):\n",
    "        # pair_dist = sim_type(X, self.similarity)\n",
    "        n_samples, n_features = X.shape\n",
    "        self.centroids = X[np.random.choice(n_samples, self.k, replace=False)]\n",
    "        temp = []\n",
    "\n",
    "        for _ in range(iterations):\n",
    "            temp = []\n",
    "            for point in X:\n",
    "                distances = np.sqrt(np.sum((self.centroids - point)**2, axis=1))\n",
    "                temp.append(np.argmin(distances))\n",
    "            temp = np.asarray(temp)\n",
    "            \n",
    "            new_centroids = np.array([np.mean(X[temp == i], axis=0) for i in range(self.k)])\n",
    "\n",
    "            if self.stop == 'increase':\n",
    "                self.sse = self.calc_sse(X, temp)\n",
    "                if self.sse > self.prev_score:\n",
    "                    break\n",
    "                self.prev_score = self.sse\n",
    "            elif self.stop == 'iter':\n",
    "                continue\n",
    "            elif np.max(np.abs(self.centroids - new_centroids)) < 0.001:\n",
    "                break\n",
    "\n",
    "            self.centroids = new_centroids\n",
    "            self.num_iter += 1\n",
    "\n",
    "        self.sse_score = self.calc_sse(X, temp)\n",
    "        self.c_labels = self.cluster_labels(temp, y)\n",
    "        return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 43\n"
     ]
    }
   ],
   "source": [
    "euclidean = KMeans(k=len(np.unique(labels)))\n",
    "euclidean_labels = euclidean.fit(data, labels)\n",
    "print(\"Iterations:\", euclidean.num_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 88\n"
     ]
    }
   ],
   "source": [
    "cosine = KMeans(k=len(np.unique(labels)))\n",
    "cosine_nums = pairwise_distances(data, metric='cosine')\n",
    "cosine_labels = cosine.fit(cosine_nums, labels)\n",
    "print(\"Iterations:\", cosine.num_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 18\n"
     ]
    }
   ],
   "source": [
    "jaccard = KMeans(k=len(np.unique(labels)))\n",
    "jaccard_nums = pairwise_distances(data, metric='hamming')\n",
    "jaccard_labels = jaccard.fit(jaccard_nums, labels)\n",
    "print(\"Iterations:\", jaccard.num_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean SSE: 25324912667.12255\n",
      "Cosine SSE: 677753.4069187804\n",
      "Jaccard SSE: 33035.760057596606\n"
     ]
    }
   ],
   "source": [
    "print(\"Euclidean SSE:\", euclidean.sse)\n",
    "print(\"Cosine SSE:\", cosine.sse)\n",
    "print(\"Jaccard SSE:\", jaccard.sse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. After comparing the SSEs of Euclidean, Cosine, and Jaccard distances, we see see that Jaccard is the best as it has the lowest SSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean Accuracy: 0.6011\n",
      "Cosine Accuracy: 0.5254\n",
      "Jaccard Accuracy: 0.3403\n"
     ]
    }
   ],
   "source": [
    "print(\"Euclidean Accuracy:\", accuracy_score(labels, euclidean.c_labels))\n",
    "print(\"Cosine Accuracy:\", accuracy_score(labels, cosine.c_labels))\n",
    "print(\"Jaccard Accuracy:\", accuracy_score(labels, jaccard.c_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Comparing the Accuracys of Euclidean, Cosine, and Jaccard, we see see that Euclidean has the highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "increased_euclidean = KMeans(k=len(np.unique(labels)), stop='increase')\n",
    "euclidean_labels = increased_euclidean.fit(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "increased_cosine = KMeans(k=len(np.unique(labels)), stop='increase')\n",
    "cosine_labels = increased_cosine.fit(cosine_nums, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "increased_jaccard = KMeans(k=len(np.unique(labels)), stop='increase')\n",
    "jaccard_labels = increased_jaccard.fit(jaccard_nums, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean = KMeans(k=len(np.unique(labels)), stop='iter')\n",
    "euclidean_labels = euclidean.fit(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine = KMeans(k=len(np.unique(labels)), stop='iter')\n",
    "cosine_labels = cosine.fit(cosine_nums, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard = KMeans(k=len(np.unique(labels)), stop='iter')\n",
    "jaccard_labels = jaccard.fit(jaccard_nums, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. The method that requires the most time and iterations to converge is when we allow the max number of iterations to complete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean SSE with Increases in SSE: 90548946962.0\n",
      "Cosine SSE with Increases in SSE: 2326627.814077913\n",
      "Jaccard SSE with Increases in SSE: 134101.51697534873\n"
     ]
    }
   ],
   "source": [
    "print(\"Euclidean SSE with Increases in SSE:\", increased_euclidean.sse)\n",
    "print(\"Cosine SSE with Increases in SSE:\", increased_cosine.sse)\n",
    "print(\"Jaccard SSE with Increases in SSE:\", increased_jaccard.sse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. Similar to the first question, the Jaccard SSE is the best even after the SSE increases in the next iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. To summarize, Jaccard had the best SSE when we don't change the centroid position or after having the SSE increase in the next iteration.\n",
    "However, the Euclidean distance had the best Accuracy when there is no change in centroid position. Finally, among the three methods, the one\n",
    "that required the most time and iterations to converge is when we allow the max number of iterations to complete."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
